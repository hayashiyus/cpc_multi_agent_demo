
# CPC‑MS マルチエージェント DEMO（リファクタ版）やさしい解説書

**対象読者**：数学・生成モデル・深層学習に詳しくない方  
**ねらい**：最新版ノートブックが「何をどこまでやっているか」を、なるべく式を少なく、直感と図解イメージで説明します。最後に将来の拡張計画もまとめます。

---

## 0. 全体像（まずはざっくり）

このデモは、複数の情報源（X / YouTube / TikTok / 新聞 / TV）から得られる**観測**を、いくつかの**共有された言葉（コードブック）**に整理していく仕組みです。  
イメージとしては、みんながバラバラに見た出来事（観測）を、それぞれが頭の中で描く要約（内部表象）に変え、それを集めて「みんなで通じる言葉（外的表現）」に揃えていく——という流れです。

この関係は次の 3 つで表せます：

- **観測** \(o\)：窓ごとのイベント数（賛成・反対・中立のカウント）や、短い要約テキスト
- **内部表象** \(z\)：要約テキストを数値化したベクトル（埋め込み）
- **外的表現** \(w\)：みんなで共有する「語彙の中心（コードブックの原型）」

数式では、理想的には次のような**生成**の考え方を置きます：

\[
p(w, z, o) \;=\; p(w)\; p(z \mid w)\; p(o \mid z).
\]

- \(p(z \mid w)\)：共有語彙 \(w\) があると、各人の内部表象 \(z\) がどう分布するか  
- \(p(o \mid z)\)：内部表象 \(z\) から、実際の観測 \(o\) がどのくらい起きやすいか

学習は、観測 \(o\) から逆向きに \(z\) と \(w\) を推定する処理だと思ってください。

---

## 1. この最新版ノートブックで**できていること**

### 1.1 データの作り方（デモ）
- 5 媒体 × 時間窓 \(T\) の**イベントカウント**（賛成/反対/中立）を **Poisson（ポアソン）分布**で生成。
- 「TikTok → X」「X → YouTube」の**ラグ効果**を弱く注入（前の窓の総量が後の窓に少し効く）。
- 各窓の短いテキストを簡単に合成し、**ハッシュ TF‑IDF** で数値化（外部 API 不要・決定的）。

> ※ 実データに差し替える前提で、構造はそのまま流用できるように作っています。

### 1.2 CPC の心臓：**混合モデル**を EM で学習
- 観測は 2 つに分けて扱います：  
  1) カウント（賛成/反対/中立）→ **Poisson**  
  2) 埋め込み（要約のベクトル）→ **Gaussian（多次元正規）**
- それらを掛け合わせた **複合尤度（Composite Likelihood）** を用いた**混合モデル**で、
  「この窓はどの語彙中心 \(w_j\) に近いか？」を**確率的に割り当て**ます。

最小限の数式（雰囲気だけ掴んでください）：

- **E‑step（割当の更新）**  
  窓 \(t\) がクラスタ \(j\) に属する確率 \(\gamma_{tj}\) は、
  \[
  \gamma_{tj} \propto \pi_j
  \times \underbrace{\prod_k \text{Pois}(c_{tk};\lambda_{jk})}_{\text{カウントの当てはまり}}
  \times \underbrace{\mathcal N(e_t; w_j, \sigma_j^2 I)}_{\text{埋め込みの当てはまり}}.
  \]
  （最後は正規化して「確率」にします）

- **M‑step（パラメータの更新）**  
  \[
  \pi_j = \frac{1}{N}\sum_t\gamma_{tj},\quad
  \lambda_{jk} = \frac{\sum_t \gamma_{tj}\,c_{tk}}{\sum_t \gamma_{tj}},\quad
  w_j = \frac{\sum_t \gamma_{tj}\,e_t}{\sum_t \gamma_{tj}},\quad
  \sigma_j^2 = \frac{\sum_t \gamma_{tj}\,\|e_t-w_j\|^2}{D\sum_t \gamma_{tj}}.
  \]

- **ドリフト対応（忘却）**：古い窓ほど重みを小さくする**指数フォーゲッティング**で、話題の変化に追従。  
- **モニタリング**：負の対数尤度（NLL）と、語彙中心の分離度（JS っぽい指標）を記録。

### 1.3 Network（MH ネーミングゲーム）
- 各クラスタの「代表的な埋め込み方向（語義方向）」に少し寄せた**新しい中心候補** \(w'_j\) を**提案**。
- **受理率**は**尤度比**だけで決めます：
  \[
  r=\min\!\left(1,\; \frac{p(\text{全部のデータ}\mid w')}{p(\text{全部のデータ}\mid w)}\right).
  \]
  直感：**「新しい中心のほうが全体に当てはまるなら採用」**というだけです。

### 1.4 因果の二段推定（IPW つき）
- 例として **TikTok（治療）→ X（目的）** を考え、X の「賛成 − 反対」を**媒介**として分解：  
  1 段目：媒介 \(M\) を治療 \(T\) と共変量で回帰  
  2 段目：目的 \(Y\) を \(T\) と \(M\) と共変量で回帰  
- **IPW（逆確率重み）**で交絡をやわらげ、
  - **総効果**（だいたいの影響）
  - **直接効果**（媒介を取り除いた純粋な影響）
  - **間接効果**（媒介を通じた影響：\(\delta\times\theta\)）
  を推定します。

> 注意：ここでは「観測データ上の近似」です。厳密な因果効果とは言い切りません。

### 1.5 Active Inference（次に何を観測すると良いか）
- 直感：**「混ざり具合（事前の不確実性）」が、**  
  **「新しい観測を見た後の混ざり具合（事後の不確実性）」**よりも大きく減るほど**価値が高い**。
- 近似式（イメージだけでOK）：
  \[
  \mathrm{EIG} \approx H(\pi)\;-\;\mathbb{E}_{x\sim p(x\mid w)}[\,H(q(z\mid x))\,].
  \]
  ここで \(H(\cdot)\) は「混ざり具合＝エントロピー」。**モンテカルロ**で見積もります。

### 1.6 エンジニアリング面
- **チェックポイント**（\(\pi,\lambda,\mu,\sigma^2\) とメタ情報）を `npz/json` で保存。  
- **Hold‑out** で**予測対数尤度**を計算し、過学習をざっくり確認。  
- モジュール分割・型ヒント・乱数固定で**再現性**と**可読性**を確保。

---

## 2. **本来は忠実に実装したいが、デモとして簡素化**している点

1. **埋め込みの表現力**：外部の高品質埋め込み（SBERT など）は使わず、**ハッシュ TF‑IDF**に留めています。  
2. **生成分布の素朴さ**：
   - カウントは **Poisson** だけ（**過分散**や**ゼロ膨張**を未考慮）。
   - 埋め込みの分布は **等方 Gaussian**（成分間の相関や非線形形状を無視）。
3. **独立仮定**：カウントと埋め込みを**独立**として掛け算しています（本来は相関構造がありうる）。  
4. **MH の提案**：
   - 「語義方向」への**一方向提案**のみ。
   - **分割/併合（split/merge）** や **温度緩和（tempering）**、**可変次元（RJ‑MCMC）** は未実装。
5. **因果の識別**：
   - DAG（因果構造）は**仮定置き**で、**探索や検定**は簡略化。
   - **操作変数（IV）**、**DoubleML**、**反実仮想検定**など本格手法は未使用。
6. **Active Inference**：将来観測の生成を**簡略**、**コスト（API、倫理、金銭）**を未導入。  
7. **非定常性**：忘却は入れたが、**語彙の誕生/死**などのルールは未導入。  
8. **スケール**：大規模ストリーム取り込み、分散学習、ベクトル DB 連携は最小限の想定に留めています。

---

## 3. 将来の課題と**具体的な修正案**（やさしく段階的に）

### 3.1 特徴・埋め込みを強化
- **なぜ**：語彙中心 \(w\) の質は、入力の表現力に強く依存。  
- **何をする**：
  - 文埋め込みを **SBERT/Instructor/ModernBERT** 等に切替（多言語 OK）。
  - 日本語では形態素解析やサブワードで**語彙の粒度**を整える。
  - トピック分布（CTM/Top2Vec）を \(z\) に加えると**解釈性**が上がる。

### 3.2 生成モデルを現実寄りに
- **なぜ**：カウントは過分散やゼロが多いことがある。埋め込みも単純な球形ではない。  
- **何をする**：
  - \(p(o\mid z)\)：**負の二項**/ **ゼロ膨張**/ **Dirichlet‑Multinomial** の併用。  
  - \(p(z\mid w)\)：**混合ガウス（対角/フル）**や**VAE** で**非線形**を表現。  
  - **変分推論（VI）** と **EM** をデータ規模に応じて切替。

### 3.3 MH ネーミングゲームの高度化
- **なぜ**：より良い提案を出せると、**受理率↑×多様性↑**で探索効率が上がる。  
- **何をする**：
  - **意味近傍**や **LLM による命名文**をベクトル化して**提案分布**に組込み。
  - **split/merge**、**温度緩和**、**RJ‑MCMC** で語彙数の可変も許容。
  - **受理則は尤度比**に固定（数理の整合性を保持）。

### 3.4 因果推定を厳密に
- **なぜ**：観測データから因果を言うには、**識別**と**検証**が鍵。  
- **何をする**：
  - **DAG 探索**：PCMCI+（時系列の条件付独立）、VAR‑LiNGAM、NOTEARS。
  - **効果推定**：**DoubleML**、**IV**、**DoWhy** で仮定を明示・検定。
  - **感度分析**・**placebo テスト**でロバスト性を点検。

### 3.5 Active Inference を実運用化
- **なぜ**：API 料金や倫理ガイドラインなど、**コストと制約**が現実には効く。  
- **何をする**：
  - 将来観測 \(x\) を尤度モデルから**サンプル**し、EIG を**MC で厳密化**。
  - **コスト関数**を導入して \( \text{EIG} - \lambda \cdot \text{cost} \) を最大化。
  - 予算制約下の**バンディット/ベイズ最適設計**でスケジューリング。

### 3.6 非定常・オンライン学習
- **なぜ**：話題や語彙は生まれては消える。  
- **何をする**：
  - **Dirichlet Process** などで**語彙の誕生/死**をモデル化。
  - **オンライン EM/VI** と **チェックポイント**で常時更新。
  - **ドリフト検知**（ADWIN 等）で忘却率を自動調整。

### 3.7 スケールと運用
- **なぜ**：数百万窓・多数媒体に拡張するには設計が必要。  
- **何をする**：
  - 取り込み：Kafka/Flink 等の**ストリーミング**。
  - 検索：FAISS/Milvus/PGVector の**ベクトル DB**。
  - 学習：Ray/Dask/Spark、GPU/AMP で**分散最適化**。
  - 監査・可観測性：**ダッシュボード、ログ、評価レポート**の自動化。

### 3.8 評価と品質保証
- **Hold‑out** による**外挿予測**、**クロスソース波及**の当たり具合。
- **Ablation**（埋め込み無し / MH 無し / Active 無し など）で寄与分析。
- 乱数や初期値による**再現性/安定性**チェック。

---

## 4. 少しだけ数式の補足（読み飛ばし OK）

### 4.1 複合尤度の EM
- **対数尤度**（定数を省略）：
  \[
  \log p(\{x_t\})=\sum_t \log \sum_{j=1}^K \pi_j \;
  \underbrace{\prod_k \text{Pois}(c_{tk};\lambda_{jk})}_{\text{カウント}}
  \underbrace{\mathcal N(e_t; w_j,\sigma_j^2 I)}_{\text{埋め込み}}.
  \]
- **E‑step**：\(\gamma_{tj}\) は各クラスタの「その窓らしさ」。
- **M‑step**：式 1.2 に記載の通り。いずれも平均・分散の「重み付き更新」になっています。

### 4.2 MH の受理率
- 提案後の対数尤度を \(\ell'\)、前の対数尤度を \(\ell\) とすると、
  \[
  r=\min\bigl(1,\exp(\ell'-\ell)\bigr).
  \]
  より「当てはまり」が良くなれば受理されやすい、という直感そのままです。

### 4.3 因果の二段推定（イメージ）
- 1 段目：\( M \leftarrow T + \text{共変量} \)  
- 2 段目：\( Y \leftarrow T + M + \text{共変量} \)  
- **間接効果** \(=\)（\(T\) が \(M\) を動かす力）×（\(M\) が \(Y\) を動かす力）  
- **IPW** は「条件が似たサンプルほど重みを揃える」イメージで、交絡の偏りを弱めます。

### 4.4 Active Inference の EIG 近似
- 事前の混ざり具合 \(H(\pi)\) が大きいほど不確実。  
- 新しい観測 \(x\) を見た後の混ざり具合 \(H(q(z\mid x))\) が小さければ、**知った価値が大きい**。

---

## 5. 実装ロードマップ（例）

- **短期（〜1ヶ月）**：埋め込みの差し替え、負の二項／ゼロ膨張の導入、Ablation と Hold‑out を自動化。  
- **中期（〜3ヶ月）**：VAE で \(p(z\mid w)\) を強化、MH の split/merge、DAG 探索＋DoubleML、EIG にコスト導入。  
- **長期（3ヶ月〜）**：オンライン学習・DP 混合、分散実行、ダッシュボード整備、データ契約・監査の常設。

---

## 6. 用語ミニ辞典

- **Poisson（ポアソン）分布**：回数データ（何件起きたか）をモデル化する素朴な分布。  
- **Gaussian（正規）分布**：平均のまわりにベル型で散らばる連続値の分布。  
- **EM（期待値最大化）**：見えない割当を E‑step で推定し、M‑step でパラメータを更新する交互最適化。  
- **尤度**：モデルがデータをどれだけ「説明できているか」を数で表したもの。  
- **IPW**：条件の違いを重みで補正して、公平な比較に近づけるテクニック。  
- **EIG**：新しい観測がどれだけ不確実性を減らしてくれるかの指標。

---

### さいごに
このデモは、**「バラバラな情報から、みんなで共有できる言葉を作る」**ことを、できるだけ小さな材料で形にしています。  
改良の余地は多いですが、土台の設計はそのまま実運用に**段階的に**拡張できます。
