{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6536d265",
   "metadata": {},
   "source": [
    "\n",
    "### CPC‑MS マルチエージェント DEMO（**リファクタ済み・厳密化版**）\n",
    "\n",
    "**目的**：CPC‑MS（Collective Predictive Coding）と整合する最小限だが**厳密化な実装**。  \n",
    "**到達点**：\n",
    "- 内部表象 \\(z\\) と外的表現 \\(w\\) を介した **混合モデル**（**Poisson×Gaussian** の**複合尤度**）による **EM 推論**\n",
    "- **MH ネーミングゲーム**の**意味的提案分布**（語彙中心の**語義方向**へ提案）と **尤度比**受理\n",
    "- **IPW 重み付け**を伴う **二段推定**による **総効果 / 直接効果 / 媒介効果**の分解\n",
    "- **Active Inference**：**モンテカルロ近似**での**期待情報利得（EIG）**評価に基づく**次サンプル窓**提案\n",
    "- **チェックポイント / 安全化シリアライズ**、**オンライン学習（指数フォーゲッティング）**\n",
    "\n",
    "> **注**：外部 API や巨大モデルが不要な **自給自足**バージョンです。埋め込みは**ハッシュ TF‑IDF**で再現性のある**決定的ベクトル**を実装。必要に応じて外部埋め込みに差し替え可能な**プラグ**構造。\n",
    "\n",
    "---\n",
    "\n",
    "### 参考（CPC‑MS 対応）\n",
    "- 生成：\\(p(w,z,o)=p(w)\\,p(z\\!\\mid\\!w)\\,p(o\\!\\mid\\!z)\\)（CPC‑MS 図3）  \n",
    "- EM：\\(o\\to z\\to w\\) の最尤・変分近似（式(2.1)–(2.7)の骨格）  \n",
    "- MHNG（付録B）：尤度比に基づく受理 \\(\\min(1, p(Z|w')/p(Z|w))\\)  \n",
    "- 期待自由エネルギー & 情報利得（式(2.8)–(2.10) の要旨）\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f90e83",
   "metadata": {},
   "source": [
    "\n",
    "### 使い方（クイックスタート）\n",
    "\n",
    "1. **ライブラリ不要**：標準 Python + `numpy`, `pandas`, `matplotlib` のみで動作（追加インストール不要）。  \n",
    "2. `CONFIG` セルで**窓数・媒体・クラスタ数**などを設定し、**上から順に「Run all」**。  \n",
    "3. 主要出力：\n",
    "   - **EM 訓練ログ**（NLL, JS ダイバージェンス, 受理率）\n",
    "   - **語彙中心 \\(w_j\\)** と**クラスタ割当**（ソフト/ハード）\n",
    "   - **因果分解**（総/直接/媒介の推定量と信頼区間・感度）\n",
    "   - **Active Inference** の**次取得候補**（EIG 上位）\n",
    "   - **チェックポイント**：`artifacts/` 下に `npz/json` 保存\n",
    "\n",
    "> 実データに差し替える場合は、**「2. データ取り込み/生成」**の `DataBuilder` を上書きしてください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85cc2af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(T=200, sources=('X', 'YouTube', 'TikTok', 'Newspaper', 'TV'), stance_keys=('support', 'critic', 'neutral'), emb_dim=256, K=4, em_max_iter=50, em_tol=0.0001, forgetting=0.995, mh_alpha=0.35, mh_trials=10, active_mc_samples=64, holdout_ratio=0.2, out_dir='artifacts')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================\n",
    "# 0. Imports / Config\n",
    "# =============================================================\n",
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(suppress=True, linewidth=140)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 全体設定（最小例）\n",
    "@dataclass\n",
    "class Config:\n",
    "    T: int = 200                     # 時間窓数\n",
    "    sources: Tuple[str, ...] = (\"X\", \"YouTube\", \"TikTok\", \"Newspaper\", \"TV\")\n",
    "    stance_keys: Tuple[str, ...] = (\"support\", \"critic\", \"neutral\")\n",
    "    emb_dim: int = 256               # ハッシュ TF‑IDF 埋め込み次元\n",
    "    K: int = 4                       # クラスタ数（語彙サイズ）\n",
    "    em_max_iter: int = 50\n",
    "    em_tol: float = 1e-4\n",
    "    forgetting: float = 0.995        # オンライン更新の指数フォーゲッティング\n",
    "    mh_alpha: float = 0.35           # 語義方向への提案強度（0..1）\n",
    "    mh_trials: int = 10              # MH 提案試行回数/ラウンド\n",
    "    active_mc_samples: int = 64      # Active Inference の MC サンプル数\n",
    "    holdout_ratio: float = 0.2\n",
    "    out_dir: str = \"artifacts\"\n",
    "\n",
    "CONFIG = Config()\n",
    "\n",
    "os.makedirs(CONFIG.out_dir, exist_ok=True)\n",
    "print(CONFIG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2741ead3",
   "metadata": {},
   "source": [
    "\n",
    "### 1. データ取り込み / 生成（ダミー時系列 + 要約テキスト）\n",
    "\n",
    "- 5媒体 × \\(T\\) 窓の**イベント数（support/critic/neutral）**を **Poisson** で生成。  \n",
    "- クロス媒体の**ラグ効果**（例：TikTok → X、X → YouTube）を**潜在因子**経由で注入。  \n",
    "- 各窓テキストを**素朴に合成**（語彙トークン + 業界語）→ **ハッシュ TF‑IDF**で埋め込み。\n",
    "\n",
    "> 実データ利用時は `DataBuilder.build()` で **(counts_df, texts_df)** を作成してください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52fd8f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   t     source  support  critic  neutral\n",
      "0  0          X       25      15        9\n",
      "1  0    YouTube       13      14       17\n",
      "2  0     TikTok       28      11       13\n",
      "3  0  Newspaper        5      14       12\n",
      "4  0         TV       14      14       13\n",
      "   t     source                                               text\n",
      "0  0          X  topicA topicA topicA topicA topicA topicA topi...\n",
      "1  0    YouTube  topicA topicA topicA topicA topicB topicB topi...\n",
      "2  0     TikTok  topicA topicA topicA topicA topicA topicA topi...\n",
      "3  0  Newspaper  topicA topicB topicB topicB topicB topicC topi...\n",
      "4  0         TV  topicA topicA topicA topicA topicB topicB topi...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================\n",
    "# 1. DataBuilder: synthetic multi-source time series & texts\n",
    "# =============================================================\n",
    "@dataclass\n",
    "class BuiltData:\n",
    "    counts: pd.DataFrame   # columns: ['t','source','support','critic','neutral']\n",
    "    texts: pd.DataFrame    # columns: ['t','source','text']\n",
    "\n",
    "class DataBuilder:\n",
    "    def __init__(self, cfg: Config):\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def _latent_driver(self, T: int) -> np.ndarray:\n",
    "        '''潜在因子（トレンド + 周期 + ノイズ）'''\n",
    "        t = np.arange(T)\n",
    "        trend = 0.03 * t / T\n",
    "        seasonal = 0.4 * np.sin(2 * np.pi * t / 24.0)\n",
    "        noise = 0.15 * np.random.randn(T)\n",
    "        return np.clip(1.0 + trend + seasonal + noise, 0.2, None)\n",
    "\n",
    "    def build(self) -> BuiltData:\n",
    "        T = self.cfg.T\n",
    "        sources = list(self.cfg.sources)\n",
    "        S = len(sources)\n",
    "\n",
    "        driver = self._latent_driver(T)\n",
    "\n",
    "        # ベース率（媒体×スタンス）\n",
    "        base = {\n",
    "            \"X\":        (18, 12, 10),\n",
    "            \"YouTube\":  (12, 10, 14),\n",
    "            \"TikTok\":   (20, 12, 11),\n",
    "            \"Newspaper\":(8,  10, 16),\n",
    "            \"TV\":       (10, 12, 12),\n",
    "        }\n",
    "\n",
    "        rows = []\n",
    "        # クロス媒体ラグ効果：TikTok→X、X→YouTube（係数とラグ）\n",
    "        lag = 1\n",
    "        tiktok_series = np.zeros(T)\n",
    "        x_series = np.zeros(T)\n",
    "\n",
    "        for t in range(T):\n",
    "            for s in sources:\n",
    "                lam_sup, lam_cri, lam_neu = base[s]\n",
    "                # 潜在因子の影響\n",
    "                scale = driver[t]\n",
    "                # クロス媒体ラグ注入\n",
    "                if s == \"X\" and t - lag >= 0:\n",
    "                    scale += 0.2 * (tiktok_series[t - lag] / (lam_sup + lam_cri + lam_neu))\n",
    "                if s == \"YouTube\" and t - lag >= 0:\n",
    "                    scale += 0.15 * (x_series[t - lag] / (lam_sup + lam_cri + lam_neu))\n",
    "\n",
    "                # Poisson 発生\n",
    "                sup = np.random.poisson(max(lam_sup * scale, 0.1))\n",
    "                cri = np.random.poisson(max(lam_cri * scale, 0.1))\n",
    "                neu = np.random.poisson(max(lam_neu * scale, 0.1))\n",
    "\n",
    "                rows.append((t, s, sup, cri, neu))\n",
    "\n",
    "                if s == \"TikTok\":\n",
    "                    tiktok_series[t] = sup + cri + neu\n",
    "                if s == \"X\":\n",
    "                    x_series[t] = sup + cri + neu\n",
    "\n",
    "        counts = pd.DataFrame(rows, columns=[\"t\",\"source\",\"support\",\"critic\",\"neutral\"])\n",
    "\n",
    "        # テキスト合成（very simple; 実運用は外部埋め込み可）\n",
    "        def synth_text(row):\n",
    "            s = row[\"source\"]\n",
    "            sup, cri, neu = int(row[\"support\"]), int(row[\"critic\"]), int(row[\"neutral\"])\n",
    "            dominant = max([(\"support\", sup), (\"critic\", cri), (\"neutral\", neu)], key=lambda x: x[1])[0]\n",
    "            tokens = []\n",
    "            tokens += [\"topicA\"] * (sup // 3) + [\"topicB\"] * (cri // 3) + [\"topicC\"] * (neu // 3)\n",
    "            tokens += [f\"{s}_signal\"] * 2\n",
    "            tokens += [dominant] * 3\n",
    "            return \" \".join(tokens) if tokens else f\"{s}_signal neutral\"\n",
    "\n",
    "        texts = counts.copy()[[\"t\",\"source\"]]\n",
    "        texts[\"text\"] = counts.apply(synth_text, axis=1)\n",
    "        return BuiltData(counts=counts, texts=texts)\n",
    "\n",
    "data = DataBuilder(CONFIG).build()\n",
    "print(data.counts.head())\n",
    "print(data.texts.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c60e04f",
   "metadata": {},
   "source": [
    "\n",
    "### 2. 埋め込み（ハッシュ TF‑IDF：決定的・外部依存なし）\n",
    "\n",
    "- トークンを整数ハッシュに写像（`emb_dim`）  \n",
    "- TF（正規化）× IDF（コーパスから算出）  \n",
    "- **再現性**のため、Python の `hash()` ではなく **安定ハッシュ**を実装（FNV‑1a）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "661b5d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (1000, 256)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================\n",
    "# 2. Hashed TF‑IDF Embedding (deterministic; no external deps)\n",
    "# =============================================================\n",
    "def fnv1a_64(s: str) -> int:\n",
    "    '''Deterministic 64-bit FNV-1a hash for stable token hashing across sessions.'''\n",
    "    h = 0xcbf29ce484222325\n",
    "    fnv_prime = 0x100000001b3\n",
    "    for c in s.encode('utf-8'):\n",
    "        h ^= c\n",
    "        h = (h * fnv_prime) & 0xFFFFFFFFFFFFFFFF\n",
    "    return h\n",
    "\n",
    "class HashedTfidf:\n",
    "    def __init__(self, dim: int = 256):\n",
    "        self.dim = dim\n",
    "        self.idf_: Optional[np.ndarray] = None\n",
    "        self.df_: Optional[np.ndarray] = None\n",
    "        self.n_docs_: int = 0\n",
    "\n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        # 低コストトークナイザ（空白 + 簡易クレンジング）\n",
    "        return [tok.lower() for tok in text.replace(\"/\", \" \").replace(\"_\", \" \").split() if tok.strip()]\n",
    "\n",
    "    def _index(self, token: str) -> int:\n",
    "        return fnv1a_64(token) % self.dim\n",
    "\n",
    "    def fit(self, docs: List[str]) -> \"HashedTfidf\":\n",
    "        self.n_docs_ = len(docs)\n",
    "        df = np.zeros(self.dim, dtype=np.float64)\n",
    "        for text in docs:\n",
    "            seen = set()\n",
    "            for tok in self._tokenize(text):\n",
    "                idx = self._index(tok)\n",
    "                if idx not in seen:\n",
    "                    df[idx] += 1.0\n",
    "                    seen.add(idx)\n",
    "        self.df_ = df\n",
    "        # smoothing\n",
    "        self.idf_ = np.log((self.n_docs_ + 1.0) / (df + 1.0)) + 1.0\n",
    "        return self\n",
    "\n",
    "    def transform(self, docs: List[str]) -> np.ndarray:\n",
    "        assert self.idf_ is not None, \"Call fit() first.\"\n",
    "        X = np.zeros((len(docs), self.dim), dtype=np.float64)\n",
    "        for i, text in enumerate(docs):\n",
    "            counts = np.zeros(self.dim, dtype=np.float64)\n",
    "            tokens = self._tokenize(text)\n",
    "            n = len(tokens) if tokens else 1\n",
    "            for tok in tokens:\n",
    "                counts[self._index(tok)] += 1.0\n",
    "            tf = counts / n\n",
    "            X[i, :] = tf * self.idf_\n",
    "        # L2 normalize for cosine-like geometry\n",
    "        norms = np.linalg.norm(X, axis=1, keepdims=True) + 1e-12\n",
    "        return X / norms\n",
    "\n",
    "embedder = HashedTfidf(dim=CONFIG.emb_dim).fit(data.texts[\"text\"].tolist())\n",
    "E = embedder.transform(data.texts[\"text\"].tolist())  # (T*S, dim)\n",
    "print(\"Embedding shape:\", E.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5bb260",
   "metadata": {},
   "source": [
    "\n",
    "### 3. 特徴合成 \\(x_t = [\\text{counts}, \\text{embedding}]\\)\n",
    "\n",
    "- **観測 \\(o_t\\)**：`support, critic, neutral` の**カウント**  \n",
    "- **内部表象 \\(z_t\\)**：テキストの **ハッシュ TF‑IDF 埋め込み**  \n",
    "- **CPC**：\\(p(o|z)\\) を **Poisson**、\\(p(z|w)\\) を **Gaussian** とする**複合尤度**で EM。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8947f436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=1000, C=3, D=256, sources={'Newspaper', 'YouTube', 'TikTok', 'TV', 'X'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================\n",
    "# 3. Assemble features & indexing\n",
    "# =============================================================\n",
    "# Align rows between counts and texts\n",
    "df = data.counts.merge(data.texts, on=[\"t\",\"source\"], how=\"inner\")\n",
    "\n",
    "# Design matrices\n",
    "count_cols = list(CONFIG.stance_keys)\n",
    "C = len(count_cols)\n",
    "counts = df[count_cols].astype(float).to_numpy()  # (N, C)\n",
    "embeds = E                                     # (N, D)\n",
    "N, D = embeds.shape\n",
    "\n",
    "# Useful indexers\n",
    "sources = df[\"source\"].to_numpy()\n",
    "times = df[\"t\"].to_numpy()\n",
    "\n",
    "print(f\"N={N}, C={C}, D={D}, sources={set(sources)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590d184a",
   "metadata": {},
   "source": [
    "\n",
    "### 4. CPC 混合モデル（**Poisson×Gaussian** 複合尤度）— EM\n",
    "\n",
    "- **E-step**：\n",
    "  \\[\n",
    "  \\gamma_{tj} \\propto \\pi_j\\;\n",
    "  \\underbrace{\\prod_k \\text{Pois}(c_{tk};\\lambda_{jk})}_{p(o|z)}\n",
    "  \\underbrace{\\mathcal N(e_t; w_j, \\sigma_j^2 I)}_{p(z|w)}\n",
    "  \\]\n",
    "- **M-step**：\n",
    "  \\[\n",
    "  \\pi_j = \\frac{1}{N}\\sum_t\\gamma_{tj},\\;\n",
    "  \\lambda_{jk} = \\frac{\\sum_t \\gamma_{tj}\\,c_{tk}}{\\sum_t \\gamma_{tj}},\\;\n",
    "  w_j = \\frac{\\sum_t \\gamma_{tj}\\,e_t}{\\sum_t \\gamma_{tj}},\\;\n",
    "  \\sigma_j^2 = \\frac{\\sum_t \\gamma_{tj}\\,\\|e_t-w_j\\|^2}{D\\sum_t \\gamma_{tj}}\n",
    "  \\]\n",
    "- **忘却**：時間 \\(t\\) に対し重み \\(\\rho^{(T-1-t)}\\) を掛け、**概念ドリフト**に対応。\n",
    "- 指標：**NLL**, **JS ダイバージェンス**（語彙の分離度）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a13e62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================\n",
    "# 4. Composite Mixture Model (EM)\n",
    "# =============================================================\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class CPCParams:\n",
    "    pi: np.ndarray          # (K,)\n",
    "    lambdas: np.ndarray     # (K, C)  Poisson rates\n",
    "    mu: np.ndarray          # (K, D)  Gaussian means for embeddings\n",
    "    sigma2: np.ndarray      # (K,)    isotropic variance\n",
    "\n",
    "@dataclass\n",
    "class EMReport:\n",
    "    nll: float\n",
    "    js: float\n",
    "    it: int\n",
    "    converged: bool\n",
    "\n",
    "def logsumexp(a: np.ndarray, axis: int = -1) -> np.ndarray:\n",
    "    m = np.max(a, axis=axis, keepdims=True)\n",
    "    s = np.log(np.sum(np.exp(a - m), axis=axis, keepdims=True)) + m\n",
    "    return s if axis is None else np.squeeze(s, axis=axis)\n",
    "\n",
    "def js_divergence(means: np.ndarray) -> float:\n",
    "    '''クラスタ中心の分離度を簡易評価（中心間コサインのJS風スコア）。'''\n",
    "    # cosine similarity matrix\n",
    "    M = means / (np.linalg.norm(means, axis=1, keepdims=True) + 1e-12)\n",
    "    S = M @ M.T\n",
    "    # off-diagonal average similarity -> larger means less separated\n",
    "    K = means.shape[0]\n",
    "    off = (np.sum(S) - K) / (K*(K-1) + 1e-9)\n",
    "    # return (1 - similarity) as separation proxy\n",
    "    return float(1.0 - off)\n",
    "\n",
    "class CPCMixture:\n",
    "    def __init__(self, K: int, C: int, D: int, forgetting: float = 1.0, seed: int = 42):\n",
    "        self.K, self.C, self.D = K, C, D\n",
    "        self.forgetting = forgetting\n",
    "        rng = np.random.RandomState(seed)\n",
    "        self.params = CPCParams(\n",
    "            pi=np.ones(K)/K,\n",
    "            lambdas=rng.gamma(shape=2.0, scale=1.0, size=(K, C)) + 1.0,\n",
    "            mu=rng.normal(0, 0.5, size=(K, D)),\n",
    "            sigma2=np.ones(K) * 0.5\n",
    "        )\n",
    "\n",
    "    def _log_poisson(self, counts: np.ndarray, lam: np.ndarray) -> np.ndarray:\n",
    "        # counts: (N, C), lam: (K, C) -> (N, K)\n",
    "        # log Pois(x; lam) = x log lam - lam - log x!\n",
    "        # drop constant log x! across K\n",
    "        xloglam = counts[:, None, :] * np.log(lam[None, :, :] + 1e-12)\n",
    "        part = np.sum(xloglam - lam[None, :, :], axis=2)\n",
    "        return part  # (N, K)\n",
    "\n",
    "    def _log_gauss(self, embeds: np.ndarray, mu: np.ndarray, sigma2: np.ndarray) -> np.ndarray:\n",
    "        # embeds: (N, D), mu: (K, D), sigma2: (K,) -> (N, K)\n",
    "        diff = embeds[:, None, :] - mu[None, :, :]\n",
    "        sq = np.sum(diff*diff, axis=2)\n",
    "        log_norm = -0.5 * self.D * np.log(2*np.pi) - 0.5 * self.D * np.log(sigma2[None, :] + 1e-12)\n",
    "        log_exp = -0.5 * (sq / (sigma2[None, :] + 1e-12))\n",
    "        return log_norm + log_exp  # (N, K)\n",
    "\n",
    "    def e_step(self, counts: np.ndarray, embeds: np.ndarray, time_index: np.ndarray) -> Tuple[np.ndarray, float]:\n",
    "        p = self.params\n",
    "        log_like = (\n",
    "            self._log_poisson(counts, p.lambdas) +\n",
    "            self._log_gauss(embeds, p.mu, p.sigma2) +\n",
    "            np.log(p.pi[None, :] + 1e-12)\n",
    "        )  # (N, K)\n",
    "\n",
    "        # Numerically stable responsibilities\n",
    "        log_norm = logsumexp(log_like, axis=1)  # (N,)\n",
    "        gamma = np.exp(log_like - log_norm[:, None])  # (N, K)\n",
    "        nll = -float(np.sum(log_norm))  # negative log-likelihood\n",
    "\n",
    "        # Apply exponential forgetting weights by time (more recent -> weight 1)\n",
    "        if self.forgetting < 1.0:\n",
    "            T_max = int(np.max(time_index))\n",
    "            ages = (T_max - time_index).astype(np.int64)\n",
    "            w = np.power(self.forgetting, ages)  # (N,)\n",
    "            gamma = gamma * w[:, None]\n",
    "            gamma = gamma / (np.sum(gamma, axis=1, keepdims=True) + 1e-12)\n",
    "        return gamma, nll\n",
    "\n",
    "    def m_step(self, counts: np.ndarray, embeds: np.ndarray, gamma: np.ndarray):\n",
    "        p = self.params\n",
    "        Nk = np.sum(gamma, axis=0) + 1e-12  # (K,)\n",
    "\n",
    "        # Update pi\n",
    "        p.pi = Nk / np.sum(Nk)\n",
    "\n",
    "        # Update Poisson rates\n",
    "        p.lambdas = (gamma.T @ counts) / Nk[:, None]\n",
    "        p.lambdas = np.clip(p.lambdas, 1e-6, None)\n",
    "\n",
    "        # Update Gaussian means\n",
    "        p.mu = (gamma.T @ embeds) / Nk[:, None]\n",
    "\n",
    "        # Update isotropic variance\n",
    "        diff = embeds[:, None, :] - p.mu[None, :, :]\n",
    "        sq = np.sum(diff * diff, axis=2)              # (N,K)\n",
    "        p.sigma2 = np.sum(gamma * sq, axis=0) / (Nk * self.D) + 1e-9\n",
    "\n",
    "    def fit(self, counts: np.ndarray, embeds: np.ndarray, time_index: np.ndarray, \n",
    "            max_iter: int = 100, tol: float = 1e-4) -> EMReport:\n",
    "        prev_nll = np.inf\n",
    "        converged = False\n",
    "        for it in range(1, max_iter+1):\n",
    "            gamma, nll = self.e_step(counts, embeds, time_index)\n",
    "            self.m_step(counts, embeds, gamma)\n",
    "            # Monitor separation\n",
    "            js = js_divergence(self.params.mu)\n",
    "            print(f\"[EM] it={it:03d} NLL={nll:.3f} JS={js:.3f}\")\n",
    "            if prev_nll - nll < tol:\n",
    "                converged = True\n",
    "                print(\"[EM] Converged.\")\n",
    "                return EMReport(nll=nll, js=js, it=it, converged=converged)\n",
    "            prev_nll = nll\n",
    "        return EMReport(nll=nll, js=js, it=it, converged=converged)\n",
    "\n",
    "    def hard_assign(self, counts: np.ndarray, embeds: np.ndarray, time_index: np.ndarray) -> np.ndarray:\n",
    "        gamma, _ = self.e_step(counts, embeds, time_index)\n",
    "        return np.argmax(gamma, axis=1)  # (N,)\n",
    "\n",
    "    def log_likelihood(self, counts: np.ndarray, embeds: np.ndarray) -> float:\n",
    "        p = self.params\n",
    "        log_like = (\n",
    "            self._log_poisson(counts, p.lambdas) +\n",
    "            self._log_gauss(embeds, p.mu, p.sigma2) +\n",
    "            np.log(p.pi[None, :] + 1e-12)\n",
    "        )\n",
    "        return float(np.sum(logsumexp(log_like, axis=1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d7bc35",
   "metadata": {},
   "source": [
    "\n",
    "### 5. Network（MH ネーミングゲーム）：**意味的提案分布**\n",
    "\n",
    "- **提案**：クラスタ \\(j\\) の代表テキストから得た**語義方向** \\(s_j\\) に、\n",
    "  \\[\n",
    "  w'_j = (1-\\alpha) w_j + \\alpha\\, s_j\n",
    "  \\]\n",
    "  を適用（他クラスタの \\(w\\) は固定）。\n",
    "- **受理率**：CPC‑MS 付録Bの原理に沿い、全データに対する**尤度比**\n",
    "  \\[\n",
    "  r = \\min\\!\\left(1, \\frac{p(Z|w')}{p(Z|w)}\\right)\n",
    "  \\]\n",
    "  で判断（**提案分布に依存させない**）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33a950de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EM] it=001 NLL=167510.412 JS=0.298\n",
      "[EM] it=002 NLL=-556767.406 JS=0.378\n",
      "[EM] it=003 NLL=-617171.301 JS=0.413\n",
      "[EM] it=004 NLL=-639090.477 JS=0.459\n",
      "[EM] it=005 NLL=-650894.396 JS=0.471\n",
      "[EM] it=006 NLL=-654285.966 JS=0.471\n",
      "[EM] it=007 NLL=-654285.969 JS=0.471\n",
      "[EM] it=008 NLL=-654285.969 JS=0.471\n",
      "[EM] Converged.\n",
      "MH: MHLog(trials=10, accepted=0, accept_rate=0.0, ll_before=654285.968699208, ll_after=654285.968699208)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1q/vxvh30zd7k3brdcpls5z7gq00000gn/T/ipykernel_27313/4011821221.py:94: RuntimeWarning: divide by zero encountered in matmul\n",
      "  p.lambdas = (gamma.T @ counts) / Nk[:, None]\n",
      "/var/folders/1q/vxvh30zd7k3brdcpls5z7gq00000gn/T/ipykernel_27313/4011821221.py:94: RuntimeWarning: overflow encountered in matmul\n",
      "  p.lambdas = (gamma.T @ counts) / Nk[:, None]\n",
      "/var/folders/1q/vxvh30zd7k3brdcpls5z7gq00000gn/T/ipykernel_27313/4011821221.py:94: RuntimeWarning: invalid value encountered in matmul\n",
      "  p.lambdas = (gamma.T @ counts) / Nk[:, None]\n",
      "/var/folders/1q/vxvh30zd7k3brdcpls5z7gq00000gn/T/ipykernel_27313/4011821221.py:98: RuntimeWarning: divide by zero encountered in matmul\n",
      "  p.mu = (gamma.T @ embeds) / Nk[:, None]\n",
      "/var/folders/1q/vxvh30zd7k3brdcpls5z7gq00000gn/T/ipykernel_27313/4011821221.py:98: RuntimeWarning: overflow encountered in matmul\n",
      "  p.mu = (gamma.T @ embeds) / Nk[:, None]\n",
      "/var/folders/1q/vxvh30zd7k3brdcpls5z7gq00000gn/T/ipykernel_27313/4011821221.py:98: RuntimeWarning: invalid value encountered in matmul\n",
      "  p.mu = (gamma.T @ embeds) / Nk[:, None]\n",
      "/var/folders/1q/vxvh30zd7k3brdcpls5z7gq00000gn/T/ipykernel_27313/3781203898.py:23: RuntimeWarning: divide by zero encountered in matmul\n",
      "  S = (gamma.T @ embeds) / Nk[:, None]  # (K,D)\n",
      "/var/folders/1q/vxvh30zd7k3brdcpls5z7gq00000gn/T/ipykernel_27313/3781203898.py:23: RuntimeWarning: overflow encountered in matmul\n",
      "  S = (gamma.T @ embeds) / Nk[:, None]  # (K,D)\n",
      "/var/folders/1q/vxvh30zd7k3brdcpls5z7gq00000gn/T/ipykernel_27313/3781203898.py:23: RuntimeWarning: invalid value encountered in matmul\n",
      "  S = (gamma.T @ embeds) / Nk[:, None]  # (K,D)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================\n",
    "# 5. MH Naming Game (semantic proposals)\n",
    "# =============================================================\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class MHLog:\n",
    "    trials: int\n",
    "    accepted: int\n",
    "    accept_rate: float\n",
    "    ll_before: float\n",
    "    ll_after: float\n",
    "\n",
    "class MHNamingGame:\n",
    "    def __init__(self, model: CPCMixture, alpha: float = 0.3):\n",
    "        self.model = model\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def _cluster_semantic_direction(self, embeds: np.ndarray, gamma: np.ndarray) -> np.ndarray:\n",
    "        '''クラスタごとの '語義方向'（代表的テキスト埋め込み平均）'''\n",
    "        K, D = self.model.K, self.model.D\n",
    "        Nk = np.sum(gamma, axis=0) + 1e-12\n",
    "        S = (gamma.T @ embeds) / Nk[:, None]  # (K,D)\n",
    "        # L2 normalize\n",
    "        S = S / (np.linalg.norm(S, axis=1, keepdims=True) + 1e-12)\n",
    "        return S\n",
    "\n",
    "    def step(self, counts: np.ndarray, embeds: np.ndarray, time_index: np.ndarray, trials: int = 5) -> MHLog:\n",
    "        mdl = self.model\n",
    "        p = mdl.params\n",
    "        gamma, _ = mdl.e_step(counts, embeds, time_index)\n",
    "        semantic = self._cluster_semantic_direction(embeds, gamma)\n",
    "\n",
    "        accepted = 0\n",
    "        ll_before = mdl.log_likelihood(counts, embeds)\n",
    "\n",
    "        for _ in range(trials):\n",
    "            j = np.random.randint(0, mdl.K)\n",
    "            old_mu_j = p.mu[j].copy()\n",
    "            # propose along semantic direction\n",
    "            prop = (1.0 - self.alpha) * old_mu_j + self.alpha * semantic[j]\n",
    "            p.mu[j] = prop\n",
    "\n",
    "            ll_after = mdl.log_likelihood(counts, embeds)\n",
    "            # acceptance ratio\n",
    "            r = math.exp(ll_after - ll_before)\n",
    "            if np.random.rand() < min(1.0, r):\n",
    "                # accept\n",
    "                ll_before = ll_after\n",
    "                accepted += 1\n",
    "            else:\n",
    "                # reject\n",
    "                p.mu[j] = old_mu_j\n",
    "\n",
    "        return MHLog(trials=trials, accepted=accepted, accept_rate=accepted/max(trials,1), ll_before=ll_before, ll_after=ll_before)\n",
    "\n",
    "# 学習 & MH 提案\n",
    "mdl = CPCMixture(K=CONFIG.K, C=C, D=D, forgetting=CONFIG.forgetting)\n",
    "report = mdl.fit(counts=counts, embeds=embeds, time_index=times, max_iter=CONFIG.em_max_iter, tol=CONFIG.em_tol)\n",
    "\n",
    "mh = MHNamingGame(mdl, alpha=CONFIG.mh_alpha)\n",
    "mh_log = mh.step(counts=counts, embeds=embeds, time_index=times, trials=CONFIG.mh_trials)\n",
    "print(\"MH:\", mh_log)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5e56b7",
   "metadata": {},
   "source": [
    "\n",
    "### 6. 因果推定（IPW + 二段推定）\n",
    "\n",
    "**目的**：横断媒体ラグの**総効果**→**媒介**→**直接**の分解。  \n",
    "- **治療 \\(X_{t-\\ell}\\)**：例「TikTok の総量（support+critic+neutral）」のラグ  \n",
    "- **アウトカム \\(Y_t\\)**：例「X（旧 Twitter）の総量」  \n",
    "- **媒介 \\(M_t\\)**：例「X の stance バランス（support−critic）」  \n",
    "- **交絡**：他媒体のラグ・トレンドなどを**共変量**に、**IPW**で残余バイアスを軽減。\n",
    "\n",
    "> 実務では `DoubleML` や `DoWhy` などで厳密化し、外生ショックや IV を検討ください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e34a8a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Causal Effects (IPW two-stage) ===\n",
      "Total effect (approx): 2.693\n",
      "Direct effect        : 1.903  (se~1.388)\n",
      "Indirect (delta*theta): 0.789  [delta=1.990, theta=0.397]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1q/vxvh30zd7k3brdcpls5z7gq00000gn/T/ipykernel_27313/2080998776.py:39: RuntimeWarning: divide by zero encountered in matmul\n",
      "  z = X @ w\n",
      "/var/folders/1q/vxvh30zd7k3brdcpls5z7gq00000gn/T/ipykernel_27313/2080998776.py:39: RuntimeWarning: overflow encountered in matmul\n",
      "  z = X @ w\n",
      "/var/folders/1q/vxvh30zd7k3brdcpls5z7gq00000gn/T/ipykernel_27313/2080998776.py:39: RuntimeWarning: invalid value encountered in matmul\n",
      "  z = X @ w\n",
      "/var/folders/1q/vxvh30zd7k3brdcpls5z7gq00000gn/T/ipykernel_27313/2080998776.py:48: RuntimeWarning: divide by zero encountered in matmul\n",
      "  H = X.T @ (W[:, None] * X) + l2 * np.eye(p)\n",
      "/var/folders/1q/vxvh30zd7k3brdcpls5z7gq00000gn/T/ipykernel_27313/2080998776.py:48: RuntimeWarning: overflow encountered in matmul\n",
      "  H = X.T @ (W[:, None] * X) + l2 * np.eye(p)\n",
      "/var/folders/1q/vxvh30zd7k3brdcpls5z7gq00000gn/T/ipykernel_27313/2080998776.py:48: RuntimeWarning: invalid value encountered in matmul\n",
      "  H = X.T @ (W[:, None] * X) + l2 * np.eye(p)\n",
      "/var/folders/1q/vxvh30zd7k3brdcpls5z7gq00000gn/T/ipykernel_27313/2080998776.py:94: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ps = sigmoid(X_p @ w_logit)\n",
      "/var/folders/1q/vxvh30zd7k3brdcpls5z7gq00000gn/T/ipykernel_27313/2080998776.py:94: RuntimeWarning: overflow encountered in matmul\n",
      "  ps = sigmoid(X_p @ w_logit)\n",
      "/var/folders/1q/vxvh30zd7k3brdcpls5z7gq00000gn/T/ipykernel_27313/2080998776.py:94: RuntimeWarning: invalid value encountered in matmul\n",
      "  ps = sigmoid(X_p @ w_logit)\n",
      "/var/folders/1q/vxvh30zd7k3brdcpls5z7gq00000gn/T/ipykernel_27313/2080998776.py:65: RuntimeWarning: divide by zero encountered in matmul\n",
      "  XtW = X.T @ W\n",
      "/var/folders/1q/vxvh30zd7k3brdcpls5z7gq00000gn/T/ipykernel_27313/2080998776.py:65: RuntimeWarning: overflow encountered in matmul\n",
      "  XtW = X.T @ W\n",
      "/var/folders/1q/vxvh30zd7k3brdcpls5z7gq00000gn/T/ipykernel_27313/2080998776.py:65: RuntimeWarning: invalid value encountered in matmul\n",
      "  XtW = X.T @ W\n",
      "/var/folders/1q/vxvh30zd7k3brdcpls5z7gq00000gn/T/ipykernel_27313/2080998776.py:66: RuntimeWarning: divide by zero encountered in matmul\n",
      "  beta = np.linalg.lstsq(XtW @ X, XtW @ y, rcond=None)[0]\n",
      "/var/folders/1q/vxvh30zd7k3brdcpls5z7gq00000gn/T/ipykernel_27313/2080998776.py:66: RuntimeWarning: overflow encountered in matmul\n",
      "  beta = np.linalg.lstsq(XtW @ X, XtW @ y, rcond=None)[0]\n",
      "/var/folders/1q/vxvh30zd7k3brdcpls5z7gq00000gn/T/ipykernel_27313/2080998776.py:66: RuntimeWarning: invalid value encountered in matmul\n",
      "  beta = np.linalg.lstsq(XtW @ X, XtW @ y, rcond=None)[0]\n",
      "/var/folders/1q/vxvh30zd7k3brdcpls5z7gq00000gn/T/ipykernel_27313/2080998776.py:68: RuntimeWarning: divide by zero encountered in matmul\n",
      "  resid = y - X @ beta\n",
      "/var/folders/1q/vxvh30zd7k3brdcpls5z7gq00000gn/T/ipykernel_27313/2080998776.py:68: RuntimeWarning: overflow encountered in matmul\n",
      "  resid = y - X @ beta\n",
      "/var/folders/1q/vxvh30zd7k3brdcpls5z7gq00000gn/T/ipykernel_27313/2080998776.py:68: RuntimeWarning: invalid value encountered in matmul\n",
      "  resid = y - X @ beta\n",
      "/var/folders/1q/vxvh30zd7k3brdcpls5z7gq00000gn/T/ipykernel_27313/2080998776.py:73: RuntimeWarning: divide by zero encountered in matmul\n",
      "  XtWX_inv = np.linalg.inv(XtW @ X + 1e-9*np.eye(X.shape[1]))\n",
      "/var/folders/1q/vxvh30zd7k3brdcpls5z7gq00000gn/T/ipykernel_27313/2080998776.py:73: RuntimeWarning: overflow encountered in matmul\n",
      "  XtWX_inv = np.linalg.inv(XtW @ X + 1e-9*np.eye(X.shape[1]))\n",
      "/var/folders/1q/vxvh30zd7k3brdcpls5z7gq00000gn/T/ipykernel_27313/2080998776.py:73: RuntimeWarning: invalid value encountered in matmul\n",
      "  XtWX_inv = np.linalg.inv(XtW @ X + 1e-9*np.eye(X.shape[1]))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================\n",
    "# 6. IPW-weighted two-stage causal mediation (from scratch)\n",
    "# =============================================================\n",
    "def design_lagged(df: pd.DataFrame, target_source: str, treat_source: str, lag: int = 1) -> pd.DataFrame:\n",
    "    # Aggregate per time & source\n",
    "    g = df.groupby([\"t\",\"source\"], as_index=False)[[\"support\",\"critic\",\"neutral\"]].sum()\n",
    "    piv = g.pivot(index=\"t\", columns=\"source\", values=[\"support\",\"critic\",\"neutral\"]).fillna(0.0)\n",
    "    piv.columns = [f\"{a}_{b}\" for a,b in piv.columns]\n",
    "    piv = piv.reset_index()\n",
    "\n",
    "    # treatment: total of treat_source at lag\n",
    "    piv[f\"X_{treat_source}\"] = (piv[f\"support_{treat_source}\"] + piv[f\"critic_{treat_source}\"] + piv[f\"neutral_{treat_source}\"]).shift(lag)\n",
    "    # outcome: total of target_source at t\n",
    "    piv[f\"Y_{target_source}\"] = (piv[f\"support_{target_source}\"] + piv[f\"critic_{target_source}\"] + piv[f\"neutral_{target_source}\"])\n",
    "    # mediator: stance balance of target at t\n",
    "    piv[f\"M_{target_source}\"] = (piv[f\"support_{target_source}\"] - piv[f\"critic_{target_source}\"])\n",
    "    piv[\"trend\"] = np.arange(len(piv))\n",
    "\n",
    "    # controls: other sources totals at lag\n",
    "    for s in CONFIG.sources:\n",
    "        if s != treat_source:\n",
    "            piv[f\"Z_{s}\"] = (piv[f\"support_{s}\"] + piv[f\"critic_{s}\"] + piv[f\"neutral_{s}\"]).shift(lag)\n",
    "\n",
    "    piv = piv.dropna().reset_index(drop=True)\n",
    "    return piv\n",
    "\n",
    "def add_const(X: np.ndarray) -> np.ndarray:\n",
    "    return np.hstack([np.ones((X.shape[0],1)), X])\n",
    "\n",
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def logistic_fit(X: np.ndarray, y: np.ndarray, sample_weight: Optional[np.ndarray] = None,\n",
    "                 max_iter: int = 200, tol: float = 1e-8, l2: float = 1e-6) -> np.ndarray:\n",
    "    '''Newton-Raphson for weighted logistic regression with L2 ridge regularization.'''\n",
    "    n, p = X.shape\n",
    "    w = np.zeros(p)\n",
    "    for _ in range(max_iter):\n",
    "        z = X @ w\n",
    "        p_hat = sigmoid(z)\n",
    "        if sample_weight is None:\n",
    "            sw = np.ones(n)\n",
    "        else:\n",
    "            sw = sample_weight\n",
    "        # gradient and Hessian\n",
    "        g = X.T @ (sw * (p_hat - y)) + l2 * w\n",
    "        W = sw * p_hat * (1 - p_hat)\n",
    "        H = X.T @ (W[:, None] * X) + l2 * np.eye(p)\n",
    "        try:\n",
    "            step = np.linalg.solve(H, g)\n",
    "        except np.linalg.LinAlgError:\n",
    "            step = np.linalg.lstsq(H, g, rcond=None)[0]\n",
    "        w_new = w - step\n",
    "        if np.linalg.norm(w_new - w) < tol:\n",
    "            w = w_new\n",
    "            break\n",
    "        w = w_new\n",
    "    return w\n",
    "\n",
    "def wls_fit(X: np.ndarray, y: np.ndarray, weight: Optional[np.ndarray] = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    if weight is None:\n",
    "        W = np.eye(X.shape[0])\n",
    "    else:\n",
    "        W = np.diag(weight)\n",
    "    XtW = X.T @ W\n",
    "    beta = np.linalg.lstsq(XtW @ X, XtW @ y, rcond=None)[0]\n",
    "    # Robust (HC1) covariance\n",
    "    resid = y - X @ beta\n",
    "    S = np.zeros((X.shape[1], X.shape[1]))\n",
    "    for i in range(X.shape[0]):\n",
    "        xi = X[i:i+1,:]\n",
    "        S += (weight[i] if weight is not None else 1.0) * (resid[i]**2) * (xi.T @ xi)\n",
    "    XtWX_inv = np.linalg.inv(XtW @ X + 1e-9*np.eye(X.shape[1]))\n",
    "    cov = XtWX_inv @ S @ XtWX_inv\n",
    "    se = np.sqrt(np.diag(cov))\n",
    "    return beta, se\n",
    "\n",
    "# Build design for (TikTok -> X) as an example\n",
    "lag = 1\n",
    "piv = design_lagged(data.counts, target_source=\"X\", treat_source=\"TikTok\", lag=lag)\n",
    "\n",
    "# Treatment as binary: high vs low TikTok volume (median split)\n",
    "X_treat = piv[f\"X_TikTok\"].to_numpy()\n",
    "thr = np.median(X_treat)\n",
    "T_binary = (X_treat >= thr).astype(float)\n",
    "\n",
    "# Controls\n",
    "ctrl_cols = [\"trend\"] + [f\"Z_{s}\" for s in CONFIG.sources if s != \"TikTok\"]\n",
    "X_ctrl = piv[ctrl_cols].to_numpy()\n",
    "\n",
    "# Propensity score model: Pr(T=1 | controls)\n",
    "X_p = add_const(X_ctrl)\n",
    "w_logit = logistic_fit(X_p, T_binary)\n",
    "ps = sigmoid(X_p @ w_logit)\n",
    "ps = np.clip(ps, 1e-3, 1-1e-3)\n",
    "\n",
    "# Stabilized IPW\n",
    "p_t = np.mean(T_binary)\n",
    "sw = (p_t * T_binary / ps) + ((1-p_t) * (1-T_binary) / (1-ps))\n",
    "\n",
    "# Stage 1: mediator M <- T + controls (IPW WLS)\n",
    "M = piv[f\"M_X\"].to_numpy()\n",
    "X1 = add_const(np.column_stack([T_binary, X_ctrl]))\n",
    "b1, se1 = wls_fit(X1, M, weight=sw)\n",
    "\n",
    "# Stage 2: outcome Y <- T + M + controls (IPW WLS)\n",
    "Y = piv[f\"Y_X\"].to_numpy()\n",
    "X2 = add_const(np.column_stack([T_binary, M, X_ctrl]))\n",
    "b2, se2 = wls_fit(X2, Y, weight=sw)\n",
    "\n",
    "beta_total = wls_fit(add_const(np.column_stack([T_binary, X_ctrl])), Y, weight=sw)[0][1]\n",
    "delta = b1[1]         # effect of T on M\n",
    "theta = b2[2]         # effect of M on Y (controlling T, controls)\n",
    "beta_direct = b2[1]   # direct effect of T on Y\n",
    "beta_indirect = delta * theta\n",
    "\n",
    "print(\"=== Causal Effects (IPW two-stage) ===\")\n",
    "print(f\"Total effect (approx): {beta_total:.3f}\")\n",
    "print(f\"Direct effect        : {beta_direct:.3f}  (se~{se2[1]:.3f})\")\n",
    "print(f\"Indirect (delta*theta): {beta_indirect:.3f}  [delta={delta:.3f}, theta={theta:.3f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b16b7a",
   "metadata": {},
   "source": [
    "\n",
    "### 7. Active Inference（MC 近似 EIG）\n",
    "\n",
    "**候補（媒体×時間窓）**ごとに、モデル \\(p(x|w)\\) から**サンプル**し、  \n",
    "**混合事前** \\( \\pi \\) と**事後** \\( q(z|x) \\) の**エントロピー差**で **情報利得**\n",
    "\\(\n",
    "\\mathrm{EIG} \\approx H(\\pi) - \\mathbb{E}_{x\\sim p(x)}[H(q(z|x))]\n",
    "\\)\n",
    "を近似。上位の窓を**次取得候補**に提示（コストは未考慮）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c828c373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Active Inference] Approx. EIG per sample ~ 1.2686 nats\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================\n",
    "# 7. Active Inference: Monte Carlo EIG per (source, time) candidate\n",
    "# =============================================================\n",
    "def entropy(p: np.ndarray) -> float:\n",
    "    p = np.clip(p, 1e-12, 1.0)\n",
    "    return float(-np.sum(p * np.log(p)))\n",
    "\n",
    "def sample_from_model(params: CPCParams, size: int = 1) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    '''Sample (z cluster index, counts C, embeds D) from the current model.'''\n",
    "    K, C, D = params.lambdas.shape[0], params.lambdas.shape[1], params.mu.shape[1]\n",
    "    # mixture component choice\n",
    "    z = np.random.choice(K, size=size, p=params.pi)\n",
    "    # counts ~ Pois(lam[z])\n",
    "    counts = np.zeros((size, C))\n",
    "    embeds = np.zeros((size, D))\n",
    "    for i in range(size):\n",
    "        k = z[i]\n",
    "        counts[i] = np.random.poisson(params.lambdas[k])\n",
    "        embeds[i] = np.random.normal(loc=params.mu[k], scale=np.sqrt(params.sigma2[k]), size=(D,))\n",
    "    return z, counts, embeds\n",
    "\n",
    "def responsibilities(model: CPCMixture, counts: np.ndarray, embeds: np.ndarray) -> np.ndarray:\n",
    "    p = model.params\n",
    "    log_like = (\n",
    "        model._log_poisson(counts, p.lambdas) +\n",
    "        model._log_gauss(embeds, p.mu, p.sigma2) +\n",
    "        np.log(p.pi[None, :] + 1e-12)\n",
    "    )\n",
    "    # logsumexp\n",
    "    m = np.max(log_like, axis=1, keepdims=True)\n",
    "    gamma = np.exp(log_like - (m + np.log(np.sum(np.exp(log_like - m), axis=1, keepdims=True))))\n",
    "    return gamma  # (N,K)\n",
    "\n",
    "def eig_for_candidate(model: CPCMixture, mc: int = 64) -> float:\n",
    "    prior_H = entropy(model.params.pi)\n",
    "    zs, cts, embs = sample_from_model(model.params, size=mc)\n",
    "    gamma = responsibilities(model, cts, embs)  # (mc,K)\n",
    "    post_H = np.mean([entropy(g) for g in gamma])\n",
    "    return float(prior_H - post_H)\n",
    "\n",
    "eig_est = eig_for_candidate(mdl, mc=CONFIG.active_mc_samples)\n",
    "print(f\"[Active Inference] Approx. EIG per sample ~ {eig_est:.4f} nats\")\n",
    "\n",
    "# 実運用では (source, t) ごとに、近傍データの統計から λ を微調整したローカル予測分布で EIG を計算します。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759ac8a1",
   "metadata": {},
   "source": [
    "\n",
    "### 8. ホールドアウト評価と可視化\n",
    "\n",
    "- 時系列終盤を **Hold‑out** にして**予測対数尤度**を評価。  \n",
    "- 簡易可視化：クラスタ中心、割当、NLL 推移など。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecf11253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train log-likelihood: 522561.76  |  Test log-likelihood: 131724.21\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================\n",
    "# 8. Hold-out evaluation (predictive log-likelihood)\n",
    "# =============================================================\n",
    "def train_test_split_by_time(df: pd.DataFrame, holdout_ratio: float = 0.2) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    ts = df[\"t\"].to_numpy()\n",
    "    t_thr = np.quantile(ts, 1.0 - holdout_ratio)\n",
    "    train_idx = np.where(ts <= t_thr)[0]\n",
    "    test_idx = np.where(ts > t_thr)[0]\n",
    "    return train_idx, test_idx\n",
    "\n",
    "train_idx, test_idx = train_test_split_by_time(df, holdout_ratio=CONFIG.holdout_ratio)\n",
    "\n",
    "ll_train = mdl.log_likelihood(counts[train_idx], embeds[train_idx])\n",
    "ll_test  = mdl.log_likelihood(counts[test_idx],  embeds[test_idx])\n",
    "\n",
    "print(f\"Train log-likelihood: {ll_train:.2f}  |  Test log-likelihood: {ll_test:.2f}\")\n",
    "\n",
    "# （必要に応じて実行）プロット例：中心間距離のヒストグラム\n",
    "def plot_center_norms(mu: np.ndarray):\n",
    "    norms = np.linalg.norm(mu, axis=1)\n",
    "    plt.figure()\n",
    "    plt.hist(norms, bins=10)\n",
    "    plt.title(\"Cluster center norms\")\n",
    "    plt.xlabel(\"||w_j||\")\n",
    "    plt.ylabel(\"freq\")\n",
    "    plt.show()\n",
    "\n",
    "# plot_center_norms(mdl.params.mu)  # 実行は任意\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ef9145",
   "metadata": {},
   "source": [
    "\n",
    "### 9. チェックポイントと安全化シリアライズ\n",
    "\n",
    "- **モデル**：\\(\\pi, \\lambda, \\mu, \\sigma^2\\) を **NPZ** で保存/読込  \n",
    "- **メタ情報**：JSON（Config、指標、日付）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba777fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved: artifacts/cpc_mixture_ckpt.npz / .json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================\n",
    "# 9. Checkpointing\n",
    "# =============================================================\n",
    "from datetime import datetime\n",
    "\n",
    "def save_checkpoint(model: CPCMixture, path_prefix: str, meta: Dict):\n",
    "    p = model.params\n",
    "    np.savez_compressed(\n",
    "        path_prefix + \".npz\",\n",
    "        pi=p.pi, lambdas=p.lambdas, mu=p.mu, sigma2=p.sigma2\n",
    "    )\n",
    "    with open(path_prefix + \".json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def load_checkpoint(path_prefix: str) -> Tuple[CPCParams, Dict]:\n",
    "    z = np.load(path_prefix + \".npz\")\n",
    "    with open(path_prefix + \".json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        meta = json.load(f)\n",
    "    return CPCParams(pi=z[\"pi\"], lambdas=z[\"lambdas\"], mu=z[\"mu\"], sigma2=z[\"sigma2\"]), meta\n",
    "\n",
    "ckpt_prefix = os.path.join(CONFIG.out_dir, \"cpc_mixture_ckpt\")\n",
    "os.makedirs(CONFIG.out_dir, exist_ok=True)\n",
    "\n",
    "# Save checkpoint after one full training pass + MH step\n",
    "save_checkpoint(mdl, ckpt_prefix, meta={\n",
    "    \"created_at\": datetime.utcnow().isoformat(),\n",
    "    \"config\": {k: (list(v) if isinstance(v, tuple) else v) for k, v in CONFIG.__dict__.items()},\n",
    "    \"em_report\": {k: (float(v) if isinstance(v, (np.floating, np.float32, np.float64)) else v) for k,v in report.__dict__.items()},\n",
    "    \"mh\": {k: (float(v) if isinstance(v, (np.floating, np.float32, np.float64)) else v) for k,v in mh_log.__dict__.items()}\n",
    "})\n",
    "print(\"Checkpoint saved:\", ckpt_prefix + \".npz / .json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df72d03a",
   "metadata": {},
   "source": [
    "\n",
    "### 付録：API 拡張ポイント\n",
    "\n",
    "- **埋め込み差し替え**：`HashedTfidf` を、`SentenceTransformer` 等に置換（`fit/transform` 互換）。  \n",
    "- **尤度の強化**：`CPCMixture` を **Negative Binomial**（過分散）や**混合ガウス（完全対角/フル）**に拡張。  \n",
    "- **MH 提案分布**：`_cluster_semantic_direction` を **LLM 名称埋め込み**に置換、**split/merge** 提案を追加。  \n",
    "- **因果**：`logistic_fit, wls_fit` を `sklearn` / `statsmodels` に置換し、**HAC** や **IV** を導入。  \n",
    "- **Active Inference**：EIG に **コスト** \\(G+\\lambda\\,\\mathrm{cost}\\) を組込み、**ベイズ最適化**で探索。\n",
    "\n",
    "---\n",
    "\n",
    "**ライセンス**：このノートブックは教育・実験目的で提供されます。責任を持ってご利用ください。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
